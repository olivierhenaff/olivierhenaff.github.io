<!DOCTYPE html>
<html>
<head>
    <title>Olivier J. Hénaff</title>
	<link rel="stylesheet" href="style.css">
	<meta charset="utf-8">
	<meta name="description" content="The personal page of Olivier Hénaff">	
</head>
<body>

	<table>
	    <tr>
	        <td><img src="content/olivierhenaff.jpg" width="219" height="260"></td>
	        <td>
				<h1>Olivier J. Hénaff </h1>
			    <p>Staff Research Scientist </p>
			    <p>Google DeepMind, London, UK </p>
	        </td>
	    </tr>
	</table>

    <p class="intro"> My research aims to understand the principles underlying biological and artificial intelligence. Humans and animals produce amazingly complex behaviors, in the face of changing environments and often with little to no supervision. This has led me to study the structure of neural representations in <a href="content/perceptual-straightening.pdf">perceptual</a> and <a href="https://www.nature.com/articles/s41467-021-25939-z.pdf">physiological</a> experiments, asking how our visual system might enable such behaviors.</p> 

    <p class="intro"> At DeepMind, I have been investigating self-supervised algorithms that extract structure from raw data, enabling <a href="https://arxiv.org/pdf/1905.09272.pdf">data-efficient image recognition</a>, behaviorally-relevant <a href="https://arxiv.org/pdf/2103.10957.pdf">scene understanding</a>, and unsupervised <a href="https://arxiv.org/pdf/2203.08777.pdf">object discovery</a> and <a href="https://arxiv.org/pdf/2210.06433.pdf">temporal correspondence</a>. More recently, I have been interested in how visual representations might structure our memory, enabling <a href="https://arxiv.org/pdf/2306.01667.pdf">fast and flexible perceptual inference</a>.</p>

    <p class="intro"> Prior to joining <a href="https://deepmind.com/">DeepMind</a>, I completed my PhD at NYU's Center for Neural Science, advised by <a href="https://www.cns.nyu.edu/~lcv/index.html">Eero Simoncelli</a>. Before starting research, I studied math and physics at <a href="https://www.polytechnique.edu/">École Polytechnique</a> and <a href="https://www.bginette.com/">Lycée Sainte Geneviève</a>.</p>


    <h2>Talks</h2>
    <ul>
    	<li> ICCV 2023 tutorial on self-supervised learning of visual representations (<a href="https://feichtenhofer.github.io/iccv2023-ssl-tutorial/">website</a>) </li>
    	<li> University of Amerstam's Deep Vision Seminar 2022, covering contrastive detection, object discovery, and video learning (<a href="https://www.youtube.com/watch?v=kVrp3-DtXDk&ab_channel=Qualcomm-UvADeepLearningSeminars">recording</a>) </li>
        <li> ICCV 2021 oral on contrastive detection (<a href="https://youtu.be/_yn3Oyq5HQI">recording</a>)
        </li>
        <li><a href="https://sites.google.com/corp/view/neurips2019-svrhm2019/">NeurIPS 2019 SVRHM Workshop</a>, covering perceptual straightening and contrastive predictive coding (<a href="https://slideslive.com/38922534/predictable-representations-in-humans-and-machines?ref=account-folder-42633-folders">recording</a>)
        </li>
    </ul>



    <h2>Publications</h2>

	<table>

	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Towards In-context Scene Understanding</p>
			    <p class="authors">Ivana Balažević, David Steiner, Nikhil Parthasarathy, Relja Arandjelović, <strong>Olivier J. Hénaff</strong></p>
			    <p class="venue">Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2023 (<strong>Spotlight</strong>) </p>
			    <p class="links"><a href="https://arxiv.org/pdf/2306.01667.pdf">PDF</a> </p>
	        </td>
	    </tr>

	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Self-supervised video pretraining yields human-aligned visual representations</p>
			    <p class="authors">Nikhil Parthasarathy, S. M. Ali Eslami, João Carreira, <strong>Olivier J. Hénaff</strong></p>
			    <p class="venue">Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2023 </p>
			    <p class="links"><a href="https://arxiv.org/pdf/2210.06433.pdf">PDF</a> </p>
	        </td>
	    </tr>

<!-- 	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods</p>
			    <p class="authors">Skanda Koppula, Yazhe Li, Evan Shelhamer, Andrew Jaegle, Nikhil Parthasarathy, Relja Arandjelovic, João Carreira, <strong>Olivier J. Hénaff</strong></p>
			    <p class="venue">Neural Information Processing Systems (<strong>NeurIPS</strong>), Workshop on Self-Supervised Learning, December 2022 </p>
			    <p class="links"><a href="https://arxiv.org/pdf/2209.15589.pdf">PDF</a> </p>
	        </td>
	    </tr> -->
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Object discovery and representation networks</p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman, João Carreira, Relja Arandjelović</p>
			    <p class="venue">European Conference on Computer Vision (<strong>ECCV</strong>), October 2022 </p>
			    <p class="links"><a href="https://arxiv.org/pdf/2203.08777.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Primary visual cortex straightens natural video trajectories</p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>*, Yoon Bai*, Julie A. Charlton, Ian Nauhaus, Eero P. Simoncelli, Robbe L. T. Goris</p>
			    <p class="venue"><strong>Nature Communications</strong>, October 2021. *equal contribution </p>
			    <p class="links"><a href="https://www.nature.com/articles/s41467-021-25939-z.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Efficient Visual Pretraining with Contrastive Detection</p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, João Carreira</p>
			    <p class="venue">International Conference on Computer Vision (<strong>ICCV</strong>), October 2021 (<strong>Oral</strong>) </p>
			    <p class="links"><a href="https://arxiv.org/pdf/2103.10957.pdf">PDF</a> &nbsp;<a href="https://github.com/deepmind/detcon">code</a> &nbsp;<a href="https://youtu.be/_yn3Oyq5HQI">ICCV oral</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Divide and Contrast: Self-supervised Learning from Uncurated Data</p>
			    <p class="authors">Yonglong Tian, <strong>Olivier J. Hénaff</strong>, Aaron van den Oord</p>
			    <p class="venue">International Conference on Computer Vision (<strong>ICCV</strong>), October 2021</p>
			    <p class="links"><a href="https://arxiv.org/pdf/2105.08054.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Data-Efficient Image Recognition with Contrastive Predictive Coding </p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami, Aaron van den Oord </p>
			    <p class="venue">International Conference on Machine Learning (<strong>ICML</strong>), July 2020 </p>
			    <p class="links"><a href="https://arxiv.org/pdf/1905.09272.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Are we done with ImageNet?</p>
			    <p class="authors">Lucas Beyer*, <strong>Olivier J. Hénaff</strong>*, Alexander Kolesnikov*, Xiaohua Zhai*, Aäron van den Oord*</p>
			    <p class="venue">Tech report, June 2020. *equal contribution</p>
			    <p class="links"><a href="https://arxiv.org/pdf/2006.07159.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Representation of visual uncertainty through neural gain variability</p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Zoe M. Boundy-Singer, Kristof Meding, Corey M. Ziemba, Robbe L. T. Goris</p>
			    <p class="venue"><strong>Nature Communications</strong>, May 2020 </p>
			    <p class="links"><a href="https://www.nature.com/articles/s41467-020-15533-0.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Perceptual straightening of natural videos </p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Robbe L. T. Goris, Eero P. Simoncelli </p>
			    <p class="venue"><strong>Nature Neuroscience</strong>, April 2019 </p>
			    <p class="links"><a href="content/perceptual-straightening.pdf">PDF</a> &nbsp;<a href="https://github.com/olivierhenaff/Perceptual-Straightening-models">code</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">Geodesics of learned representations </p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Eero P. Simoncelli </p>
			    <p class="venue">International Conference on Learning Representations (<strong>ICLR</strong>), May 2016 </p>
			    <p class="links"><a href="https://arxiv.org/pdf/1511.06394.pdf">PDF</a> </p>
	        </td>
	    </tr>
	    <tr>
	        <td><div class="rectangle"></div></td>
	        <td>
				<p class="title">The local low-dimensionality of natural images </p>
			    <p class="authors"><strong>Olivier J. Hénaff</strong>, Johannes Ballé, Neil C. Rabinowitz, Eero P. Simoncelli </p>
			    <p class="venue">International Conference on Learning Representations (<strong>ICLR</strong>), May 2015 (<strong>Oral</strong>) </p>
			    <p class="links"><a href="https://arxiv.org/pdf/1412.6626.pdf">PDF</a> </p>
	        </td>
	    </tr>
	</table>
    <h2>More info</h2>
    <h3><a href="https://scholar.google.com/citations?user=Sx75CVsAAAAJ&hl=en">Google Scholar</a> / <a href="https://twitter.com/olivierhenaff">Twitter</a> / <a href="https://www.linkedin.com/in/olivier-henaff-508b27171/">LinkedIn</a> </h3> 

</body>
</html>